% main.tex, to be used with thesis.tex
% This contains countervailing computations and perhaps other notes for part 2


\chapter{Countervailing}

These are the payoffs for Langlois
$\left(
	\begin{array}{cc}
		0 & -2 \\
		1 & -1
	\end{array}
\right)
$
and Axelrod
$
\left(
	\begin{array}{cc}
		3 & 0 \\
		5 & 1
	\end{array}
\right)
$
values of
$
\left(
	\begin{array}{cc}
		R & S \\
		T & P
	\end{array}
\right)
$

\begin{align*}
&(1-x_i,\enspace x_i) 
\left(
	\begin{array}{cc}
		R & S \\
		T & P
	\end{array}
\right)
\left(
	\begin{array}{c}
	1-x_j \\
	x_j
	\end{array}
\right)
\\
= &(1-x_i)(1-x_j) R + (1-x_i)x_j S + x_i(1-x_j)T + x_i x_j P
\\
= &(1 - x_i - x_j + x_i x_j)R + (x_j - x_i x_j)S + (x_i - x_i x_j)T + x_i x_j P
\\
= &R + (T-R)x_i + (S-R)x_j  + (R - S - T + P)x_ix_j \\
=& \begin{cases}
	x_i - 2x_j			& \textrm{Langlois}	\\
	3 + 2x_i - 3x_j - x_i x_j 	&\textrm{Axelrod}
      \end{cases}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Question about discounting}

In the derivation of Zero Determinant strategies, Press and Dyson use the transition probability matrix 
\[
\left[
\begin{array}{c c c c}
p_1q_1 & p_1 (1-q_1) & (1-p_1)q_1 & (1-p_1)(1-q_1) \\
p_2q_3 & p_2 (1-q_3) & (1-p_2)q_3 & (1-p_2)(1-q_3) \\
p_3q_2 & p_3 (1-q_2) & (1-p_3)q_2 & (1-p_3)(1-q_2) \\
p_4q_4 & p_4 (1-q_4) & (1-p_4)q_4 & (1-p_4)(1-q_4)
\end{array}
\right]
\]
for IPD with mixed strategies $\mathbf{p} = (p_1, p_2, p_3, p_4), \mathbf{q} = (q_1, q_2, q_3, q_4)$ for players $X, Y$ respectively. If $\mathbf{v}$ is the stationary distribution of the above Markov chain, then they show that the dot product of $\mathbf{v}$ with an arbitrary vector $\mathbf{f}$ is given by the determinant
\[
\mathbf{v}\cdot \mathbf{f} \equiv D(\mathbf{p}, \mathbf{q}, \mathbf{f})
= \det
\left[
\begin{array}{c c c c}
-1 + p_1 q_1    &	-1 + p_1	&	-1 + q_1	&	f_1 \\
p_2 q_3         &	-1 + p_2	&	-q_3		&	f_2 \\
p_3 q_2 		&	p_3			&	-1 + q_2	&	f_3 \\
p_4 q_4			&	p_4			&	q_4			&	f_4
\end{array}
\right],
\]
so if $\mathbf{S}_X = (R, S, T, P)$, then on average $X$'s payoff is $\mathbf{v}\cdot\mathbf{S}_X = D(\mathbf{p}, \mathbf{q}, \mathbf{S}_X)$. Press and Dyson use this average payoff for their definitions and proofs, so in their game there is no discounting.

\textitbf{Question: }I'm not sure if this is a problem when comparing ZD strategies with countervailing strategies, which use a discount factor in their definition.

One way I could see doing this is making the game have five states: $(cc, cd, dc, dd, \textrm{sink})$, where sink is the end of game state associated with payoff of 0 for both players. I'm not sure what will happen to the ZD strategies if they are defined on this 5-state Markov chain.

The other way would be to say that if $X$'s average payoff is $S_X = \mathbf{v}\cdot\mathbf{S}_X$, then $X$'s discounted payoff should be $\frac{S_X}{1-w}$.

Is either of these approaches reasonable?

\subsubsection{Are countervailing strategies ZD? Using your example.}

The payoffs for $S_X$ and $S_Y$ are given by
\begin{align*}
S_X =
\frac{\mathbf{v} \cdot \mathbf{S}_X}{\mathbf{v} \cdot \mathbf{1}} =
\frac{D(\mathbf{p}, \mathbf{q}, \mathbf{S}_X)}{D(\mathbf{p}, \mathbf{q}, \mathbf{1})}
\\
\\
S_Y =
\frac{\mathbf{v} \cdot \mathbf{S}_Y}{\mathbf{v} \cdot \mathbf{1}} =
\frac{D(\mathbf{p}, \mathbf{q}, \mathbf{S}_Y)}{D(\mathbf{p}, \mathbf{q}, \mathbf{1})}
\end{align*}
so a linear (affine?) combination of payoffs is given by
\[
\alpha S_X + \beta S_Y + \gamma
=
\frac{D(\mathbf{p}, \mathbf{q}, \alpha\mathbf{S}_X + \beta\mathbf{S}_Y + \gamma \mathbf{1})}{D(\mathbf{p}, \mathbf{q}, \mathbf{1})}
\]
from the countervailing example with $\psi_j (x_i, x_j) = \frac{x_i + x_j}{2.7}$ yielding $\mathbf{p} = (0, \frac{1}{2.7}, \frac{1}{2.7}, \frac{2}{2.7})^T$
\[
\left(
\renewcommand{\arraystretch}{1.3}
\begin{array}{c}
0 \\
\frac{1}{2.7}\\
\frac{1}{2.7}\\
\frac{2}{2.7}
\end{array}
\right)
=
\alpha
\left(
\renewcommand{\arraystretch}{1.3}
\begin{array}{r}
0 \\
-2 \\
1\\
-1
\end{array}
\right)
+
\beta
\left(
\renewcommand{\arraystretch}{1.3}
\begin{array}{r}
0 \\
1 \\
-2\\
-1
\end{array}
\right)
+
\left(
\renewcommand{\arraystretch}{1.3}
\begin{array}{r}
\gamma \\
\gamma \\
\gamma \\
\gamma
\end{array}
\right)
\]
solving for $\alpha$, $\beta$, $\gamma$, we get $\alpha = \beta = -\frac{1}{2.7}$, $\gamma = 0$,
so the strategy $\mathbf{p}$ is the ZD strategy that enforces the relationship $-\frac{1}{2.7}S_X - \frac{1}{2.7}S_Y = 0$ or simply $S_X + S_Y = 0 = R$.

\subsubsection{Langlois' Countervailing example with Axelrod's R, S, T, P values}

Taking $g_i(x_j) = - \gamma_i x_j$ does not work out. Instead here's an attempt with $g_i(x_j) = R - \gamma_i x_j = 3 - \gamma_i x_j$, this way it is consistent with $Langlois'$ example, since $R = 0$ in Langlois' definition and $R = 3$ in Axelrod's definition.

Then we have
\[
g_i (x_j) = 3 - \gamma_i x_j
\]
solving the countervailing equation
\[
3 - \gamma_i x_j = 3 + 2 x_i - 3 x_j - x_i x_j - \delta \gamma_i \psi_j
\]
which yields
\[
\psi_j (x_i, x_j) = \frac{2x_i + (\gamma_i - 3) x_j - x_i x_j}{\delta \gamma_i}
\]
with the requirement that
\[
\psi_j  \in [0, 1]
\]
setting $x_j = 0$ we get $\gamma_i > 0$, and setting $x_i = 0$ gives us $(\gamma_i - 3) > 0$ or $\gamma_i > 3$. To solve $2 x_i + (\gamma_i - 3) x_j - x_i x_j \leq \delta \gamma_i$, we see that $\frac{\partial\textrm{LHS}}{\partial x_i} \geq 0$, so we can take $x_i = 1$, but $\frac{\partial\textrm{LHS}}{\partial x_j} \geq 0$ needs $\gamma_i \geq 4$. With that restriction, we can also take $x_j = 1$, and we get
\[
\frac{2 + (\gamma_i - 3) -1}{\delta \gamma_i} \leq 1
\]
or
\[
\frac{2}{1-\delta} \geq \gamma_i.
\]
Thus
\[
\psi(x_i, x_j) \in [0, 1] \textrm{ whenever } 4 \leq \gamma_i \leq \frac{2}{1-\delta}.
\]
Taking $\delta = .9$, and $\gamma_i = 5$, we get
\[
\psi_j (x_i, x_j) = \frac{2x_i + 2x_j - x_i x_j}{4.5}
\]
